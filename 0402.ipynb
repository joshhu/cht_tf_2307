{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5ndFewMEB_X"
      },
      "source": [
        "## 神經網路的資料表示法：張量Tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5m1_vFOaEB_Y"
      },
      "source": [
        "### 純量(0D 張量）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKywxUoPEB_Y",
        "outputId": "62d3a5eb-3a95-4d00-a84d-e23169e4bb90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "x = np.array(12)\n",
        "print(x)\n",
        "x.ndim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reKND-vzEB_Y"
      },
      "outputs": [],
      "source": [
        "x.ndim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-na8WG7yEB_Z"
      },
      "source": [
        "###  向量(1D 張量）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JspPp7c1EB_Z",
        "outputId": "1b38568c-1df6-4617-aa18-63d3ab9b5d62"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([12,  3,  6, 14,  7])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = np.array([12, 3, 6, 14, 7])\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjXfQtwGEB_Z",
        "outputId": "e5a78584-9e5d-48cd-b785-19a321e2f56a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.ndim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPN4LKODEB_a"
      },
      "source": [
        "###  矩陣(2D 張量）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIgYOUiGEB_a",
        "outputId": "b8694675-4058-4869-8e53-81719c10f364"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = np.array([[5, 78, 2, 34, 0],\n",
        "        [6, 79, 3, 35, 1],\n",
        "        [7, 80, 4, 36, 2]])\n",
        "x.ndim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAtL6p_pEB_a"
      },
      "source": [
        "###  3D張量和高階張量"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bn5rgPvYEB_b",
        "outputId": "ba83cea7-8e49-4880-db10-ed858334b595"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = np.array([[[5, 78, 2, 34, 0],\n",
        "        [6, 79, 3, 35, 1],\n",
        "        [7, 80, 4, 36, 2]],\n",
        "        [[5, 78, 2, 34, 0],\n",
        "        [6, 79, 3, 35, 1],\n",
        "        [7, 80, 4, 36, 2]],\n",
        "        [[5, 78, 2, 34, 0],\n",
        "        [6, 79, 3, 35, 1],\n",
        "        [7, 80, 4, 36, 2]]])\n",
        "x.ndim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjT2uDEREB_b"
      },
      "source": [
        "### 張量的關鍵屬性"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2kZg5V0EB_b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c429a825-5fb1-4235-d64f-75aaa8af3d00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aGNCPWhEB_b",
        "outputId": "87e2011b-b92e-4dda-eb6b-88e515dc4143"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_images.ndim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYElJH-wEB_c",
        "outputId": "10b2569e-4dbf-479f-a176-3350597f76ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmgCzOX0EB_c",
        "outputId": "af1b05a8-c478-48a4-d6d4-d4580de7f405"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dtype('uint8')"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_images.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY2Qj2axEB_c"
      },
      "source": [
        "**Displaying the fourth digit**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTD93kd7EB_c",
        "outputId": "550ca391-479f-478d-b03a-0f46c7b98d9d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "digit = train_images[4]\n",
        "plt.imshow(digit, cmap=plt.cm.binary)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kf3ZvwCEB_d",
        "outputId": "4469a687-0b7f-4d25-d5f0-ff124003b9ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_labels[4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqOZJ27jEB_d"
      },
      "source": [
        "### 在NumPy做張量切片 (Tensor Slicing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45Amt_59EB_d"
      },
      "outputs": [],
      "source": [
        "my_slice = train_images[10:100]\n",
        "my_slice.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljnncJ_qEB_e"
      },
      "outputs": [],
      "source": [
        "my_slice = train_images[10:100, :, :]\n",
        "my_slice.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d58r02RREB_e"
      },
      "outputs": [],
      "source": [
        "my_slice = train_images[10:100, 0:28, 0:28]\n",
        "my_slice.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsDDxXCwEB_e"
      },
      "outputs": [],
      "source": [
        "my_slice = train_images[:, 14:, 14:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NakIjwShEB_e"
      },
      "outputs": [],
      "source": [
        "vmy_slice = train_images[:, 7:-7, 7:-7]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15M36d_DAvQ-"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac7a0mToEB_e"
      },
      "source": [
        "### 資料批次 (batch) 的概念"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFIepJioEB_f"
      },
      "outputs": [],
      "source": [
        "batch = train_images[:128]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUooK_I8EB_f"
      },
      "outputs": [],
      "source": [
        "batch = train_images[128:256]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSjSo7bXEB_f"
      },
      "outputs": [],
      "source": [
        "n = 3\n",
        "batch = train_images[128 * n:128 * (n + 1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZqVzhacEB_h"
      },
      "source": [
        "## 神經網路的工具：張量運算"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vi5pQGP9EB_h"
      },
      "source": [
        "### 逐元素的運算"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 手刻一個RELU"
      ],
      "metadata": {
        "id": "UzCEjsaXBty0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXEZCJdeEB_h"
      },
      "outputs": [],
      "source": [
        "def naive_relu(x):\n",
        "    assert len(x.shape) == 2\n",
        "    x = x.copy()\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(x.shape[1]):\n",
        "            x[i, j] = max(x[i, j], 0)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48wzl1S7EB_i"
      },
      "outputs": [],
      "source": [
        "def naive_add(x, y):\n",
        "    assert len(x.shape) == 2\n",
        "    assert x.shape == y.shape\n",
        "    x = x.copy()\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(x.shape[1]):\n",
        "            x[i, j] += y[i, j]\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51037ba8-65c1-496a-d467-3c07fe0ced52",
        "id": "dIhrwJf0Bjf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Took: 22.40 s\n"
          ]
        }
      ],
      "source": [
        "t0 = time.time()\n",
        "for _ in range(10000):\n",
        "    z = naive_add(x, y)\n",
        "    z = naive_relu(z)\n",
        "print(\"Took: {0:.2f} s\".format(time.time() - t0))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 用numpy建立一個relu"
      ],
      "metadata": {
        "id": "bQ65rehdBxAN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1K16VWoLEB_j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f5400ea-aea8-4d82-d63c-bda6df9017e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Took: 0.07 s\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "x = np.random.random((20, 100))\n",
        "y = np.random.random((20, 100))\n",
        "\n",
        "t0 = time.time()\n",
        "for _ in range(10000):\n",
        "    z = x + y\n",
        "    z = np.maximum(z, 0.)\n",
        "print(\"Took: {0:.2f} s\".format(time.time() - t0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zbXqL_-EB_k"
      },
      "source": [
        "### 張量擴張 (Broadcasting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-Zbnd5HEB_k"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "X = np.random.random((32, 10))\n",
        "y = np.random.random((10,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5AitC1EEB_k"
      },
      "outputs": [],
      "source": [
        "y = np.expand_dims(y, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXGXgH_cEB_k"
      },
      "outputs": [],
      "source": [
        "Y = np.concatenate([y] * 32, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgDcfZy3EB_l"
      },
      "outputs": [],
      "source": [
        "def naive_add_matrix_and_vector(x, y):\n",
        "    assert len(x.shape) == 2\n",
        "    assert len(y.shape) == 1\n",
        "    assert x.shape[1] == y.shape[0]\n",
        "    x = x.copy()\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(x.shape[1]):\n",
        "            x[i, j] += y[j]\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTaosx9aEB_l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5110f648-9cfd-46d4-aa66-389b3e9f7c97"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(64, 3, 32, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "import numpy as np\n",
        "x = np.random.random((64, 3, 32, 10))\n",
        "y = np.random.random((32, 10))\n",
        "z = np.maximum(x, y)\n",
        "z.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MI4h03l6EB_m"
      },
      "source": [
        "### 張量點積運算"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRmZKFqvEB_m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8074510d-dd42-4f40-8812-f6432b04a9c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 16)\n"
          ]
        }
      ],
      "source": [
        "x = np.random.random((32,2))\n",
        "y = np.random.random((2,16))\n",
        "z = np.dot(x, y)\n",
        "print(z.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJJVA1lYEB_m"
      },
      "outputs": [],
      "source": [
        "def naive_vector_dot(x, y):\n",
        "    assert len(x.shape) == 1\n",
        "    assert len(y.shape) == 1\n",
        "    assert x.shape[0] == y.shape[0]\n",
        "    z = 0.\n",
        "    for i in range(x.shape[0]):\n",
        "        z += x[i] * y[i]\n",
        "    return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRVtNaouEB_n"
      },
      "outputs": [],
      "source": [
        "def naive_matrix_vector_dot(x, y):\n",
        "    assert len(x.shape) == 2\n",
        "    assert len(y.shape) == 1\n",
        "    assert x.shape[1] == y.shape[0]\n",
        "    z = np.zeros(x.shape[0])\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(x.shape[1]):\n",
        "            z[i] += x[i, j] * y[j]\n",
        "    return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RqXBIyJEB_n"
      },
      "outputs": [],
      "source": [
        "def naive_matrix_vector_dot(x, y):\n",
        "    z = np.zeros(x.shape[0])\n",
        "    for i in range(x.shape[0]):\n",
        "        z[i] = naive_vector_dot(x[i, :], y)\n",
        "    return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L11dGUrREB_n"
      },
      "outputs": [],
      "source": [
        "def naive_matrix_dot(x, y):\n",
        "    assert len(x.shape) == 2\n",
        "    assert len(y.shape) == 2\n",
        "    assert x.shape[1] == y.shape[0]\n",
        "    z = np.zeros((x.shape[0], y.shape[1]))\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(y.shape[1]):\n",
        "            row_x = x[i, :]\n",
        "            column_y = y[:, j]\n",
        "            z[i, j] = naive_vector_dot(row_x, column_y)\n",
        "    return z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnPmUD6sEB_n"
      },
      "source": [
        "### 張量重塑"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZxCIA3hEB_o"
      },
      "outputs": [],
      "source": [
        "train_images = train_images.reshape((60000, 28 * 28))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tt3o3XiJEB_o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0166eeb-2bb8-4492-d647-9fac38e19e22"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "x = np.array([[0., 1.],\n",
        "        [2., 3.],\n",
        "        [4., 5.]])\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72vlCGd2EB_o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1efc090b-fc12-4444-db68-eb66363124aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.],\n",
              "       [1.],\n",
              "       [2.],\n",
              "       [3.],\n",
              "       [4.],\n",
              "       [5.]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "x = x.reshape((6, 1))\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UtKGPHIEB_o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20d5fdb5-bb74-47c9-ac77-1afd2b42e629"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "x = np.zeros((300, 20))\n",
        "x = np.transpose(x)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv0qB0bhEB_p"
      },
      "source": [
        "## 神經網路的引擎：以梯度為基礎的最佳化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srqC1XicEB_q"
      },
      "source": [
        "### 連鎖導數：反向傳播 Backpropagation 演算法"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRnx4R8GEB_r"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "x = tf.Variable(0.) # 實例化一個純量其初始值為0\n",
        "with tf.GradientTape() as tape: # 宣告Gradient tape運作的範圍\n",
        "    y = 2 * x + 3 # 在作用範圍內對張量進行操作\n",
        "grad_of_y_wrt_x = tape.gradient(y, x) # 求變數的梯度"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xruLZv7kEB_r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7ac9480-caef-4003-9432-f9dacab0aad0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 2)\n"
          ]
        }
      ],
      "source": [
        "# 在張量上操作\n",
        "\n",
        "x = tf.Variable(tf.random.uniform((2, 2)))\n",
        "print(x.shape)\n",
        "with tf.GradientTape() as tape:\n",
        "    y = 2 * x + 3\n",
        "grad_of_y_wrt_x = tape.gradient(y, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvPpSPeNEB_r"
      },
      "outputs": [],
      "source": [
        "# 進行點積操作並求gradient\n",
        "\n",
        "W = tf.Variable(tf.random.uniform((2, 2)))\n",
        "b = tf.Variable(tf.zeros((2,)))\n",
        "x = tf.random.uniform((2, 2))\n",
        "with tf.GradientTape() as tape:\n",
        "    y = tf.matmul(x, W) + b\n",
        "grad_of_y_wrt_W_and_b = tape.gradient(y, [W, b])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWFEwGsBEB_r"
      },
      "source": [
        "## 手刻一個神經網路\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34n4Bxi7EB_s"
      },
      "outputs": [],
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype(\"float32\") / 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btPLnuGKEB_s"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential([\n",
        "    layers.Dense(512, activation=\"relu\"),\n",
        "    layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euHQ7m_xEB_s"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=\"rmsprop\",\n",
        "       loss=\"sparse_categorical_crossentropy\",\n",
        "       metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "202wY7VnEB_s"
      },
      "outputs": [],
      "source": [
        "model.fit(train_images, train_labels, epochs=5, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhId3BLaEB_t"
      },
      "source": [
        "### 從頭開始重新建構模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZYE8rmOEB_v"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class NaiveDense:\n",
        "    def __init__(self, input_size, output_size, activation):\n",
        "        self.activation = activation\n",
        "\n",
        "        w_shape = (input_size, output_size)\n",
        "        w_initial_value = tf.random.uniform(w_shape, minval=0, maxval=1e-1)\n",
        "        self.W = tf.Variable(w_initial_value)\n",
        "\n",
        "        b_shape = (output_size,)\n",
        "        b_initial_value = tf.zeros(b_shape)\n",
        "        self.b = tf.Variable(b_initial_value)\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        return self.activation(tf.matmul(inputs, self.W) + self.b)\n",
        "\n",
        "    @property\n",
        "    def weights(self):\n",
        "        return [self.W, self.b]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3aW7h4VEB_v"
      },
      "outputs": [],
      "source": [
        "class NaiveSequential:\n",
        "    def __init__(self, layers):\n",
        "        self.layers = layers\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        x = inputs\n",
        "        for layer in self.layers:\n",
        "           x = layer(x)\n",
        "        return x\n",
        "\n",
        "    @property\n",
        "    def weights(self):\n",
        "       weights = []\n",
        "       for layer in self.layers:\n",
        "           weights += layer.weights\n",
        "       return weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DhNl-t1EB_w"
      },
      "outputs": [],
      "source": [
        "model = NaiveSequential([\n",
        "    NaiveDense(input_size=28 * 28, output_size=512, activation=tf.nn.relu),\n",
        "    NaiveDense(input_size=512, output_size=10, activation=tf.nn.softmax)\n",
        "])\n",
        "assert len(model.weights) == 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9jQsfhEEB_w"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "class BatchGenerator:\n",
        "    def __init__(self, images, labels, batch_size=128):\n",
        "        assert len(images) == len(labels)\n",
        "        self.index = 0\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.num_batches = math.ceil(len(images) / batch_size)\n",
        "\n",
        "    def next(self):\n",
        "        images = self.images[self.index : self.index + self.batch_size]\n",
        "        labels = self.labels[self.index : self.index + self.batch_size]\n",
        "        self.index += self.batch_size\n",
        "        return images, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stvm8hhJEB_w"
      },
      "source": [
        "### 執行單次的訓練"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Rn_mhvUEB_w"
      },
      "outputs": [],
      "source": [
        "def one_training_step(model, images_batch, labels_batch):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(images_batch)\n",
        "        per_sample_losses = tf.keras.losses.sparse_categorical_crossentropy(\n",
        "            labels_batch, predictions)\n",
        "        average_loss = tf.reduce_mean(per_sample_losses)\n",
        "    gradients = tape.gradient(average_loss, model.weights)\n",
        "    update_weights(gradients, model.weights)\n",
        "    return average_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXsQVvk6EB_x"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "\n",
        "def update_weights(gradients, weights):\n",
        "    for g, w in zip(gradients, weights):\n",
        "        w.assign_sub(g * learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pI4Z4K-PEB_x"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import optimizers\n",
        "\n",
        "optimizer = optimizers.SGD(learning_rate=1e-3)\n",
        "\n",
        "def update_weights(gradients, weights):\n",
        "    optimizer.apply_gradients(zip(gradients, weights))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLJWkfYkEB_x"
      },
      "source": [
        "### 完整的訓練循環"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uI8KL6lbEB_x"
      },
      "outputs": [],
      "source": [
        "def fit(model, images, labels, epochs, batch_size=128):\n",
        "    for epoch_counter in range(epochs):\n",
        "        print(f\"Epoch {epoch_counter}\")\n",
        "        batch_generator = BatchGenerator(images, labels)\n",
        "        for batch_counter in range(batch_generator.num_batches):\n",
        "            images_batch, labels_batch = batch_generator.next()\n",
        "            loss = one_training_step(model, images_batch, labels_batch)\n",
        "            if batch_counter % 100 == 0:\n",
        "                print(f\"loss at batch {batch_counter}: {loss:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITe8HSqGEB_y",
        "outputId": "bb01a4c9-2ad3-491e-d681-4040c6dd4864"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "loss at batch 0: 0.44\n",
            "loss at batch 100: 0.43\n",
            "loss at batch 200: 0.38\n",
            "loss at batch 300: 0.46\n",
            "loss at batch 400: 0.55\n",
            "Epoch 1\n",
            "loss at batch 0: 0.44\n",
            "loss at batch 100: 0.42\n",
            "loss at batch 200: 0.37\n",
            "loss at batch 300: 0.45\n",
            "loss at batch 400: 0.54\n",
            "Epoch 2\n",
            "loss at batch 0: 0.43\n",
            "loss at batch 100: 0.41\n",
            "loss at batch 200: 0.36\n",
            "loss at batch 300: 0.44\n",
            "loss at batch 400: 0.54\n",
            "Epoch 3\n",
            "loss at batch 0: 0.42\n",
            "loss at batch 100: 0.40\n",
            "loss at batch 200: 0.35\n",
            "loss at batch 300: 0.43\n",
            "loss at batch 400: 0.53\n",
            "Epoch 4\n",
            "loss at batch 0: 0.41\n",
            "loss at batch 100: 0.39\n",
            "loss at batch 200: 0.35\n",
            "loss at batch 300: 0.43\n",
            "loss at batch 400: 0.53\n",
            "Epoch 5\n",
            "loss at batch 0: 0.40\n",
            "loss at batch 100: 0.39\n",
            "loss at batch 200: 0.34\n",
            "loss at batch 300: 0.42\n",
            "loss at batch 400: 0.52\n",
            "Epoch 6\n",
            "loss at batch 0: 0.40\n",
            "loss at batch 100: 0.38\n",
            "loss at batch 200: 0.33\n",
            "loss at batch 300: 0.41\n",
            "loss at batch 400: 0.52\n",
            "Epoch 7\n",
            "loss at batch 0: 0.39\n",
            "loss at batch 100: 0.37\n",
            "loss at batch 200: 0.33\n",
            "loss at batch 300: 0.41\n",
            "loss at batch 400: 0.51\n",
            "Epoch 8\n",
            "loss at batch 0: 0.39\n",
            "loss at batch 100: 0.36\n",
            "loss at batch 200: 0.32\n",
            "loss at batch 300: 0.40\n",
            "loss at batch 400: 0.51\n",
            "Epoch 9\n",
            "loss at batch 0: 0.38\n",
            "loss at batch 100: 0.36\n",
            "loss at batch 200: 0.32\n",
            "loss at batch 300: 0.40\n",
            "loss at batch 400: 0.50\n",
            "Epoch 10\n",
            "loss at batch 0: 0.38\n",
            "loss at batch 100: 0.35\n",
            "loss at batch 200: 0.31\n",
            "loss at batch 300: 0.39\n",
            "loss at batch 400: 0.50\n",
            "Epoch 11\n",
            "loss at batch 0: 0.37\n",
            "loss at batch 100: 0.35\n",
            "loss at batch 200: 0.31\n",
            "loss at batch 300: 0.39\n",
            "loss at batch 400: 0.50\n",
            "Epoch 12\n",
            "loss at batch 0: 0.37\n",
            "loss at batch 100: 0.34\n",
            "loss at batch 200: 0.31\n",
            "loss at batch 300: 0.39\n",
            "loss at batch 400: 0.49\n",
            "Epoch 13\n",
            "loss at batch 0: 0.36\n",
            "loss at batch 100: 0.34\n",
            "loss at batch 200: 0.30\n",
            "loss at batch 300: 0.38\n",
            "loss at batch 400: 0.49\n",
            "Epoch 14\n",
            "loss at batch 0: 0.36\n",
            "loss at batch 100: 0.33\n",
            "loss at batch 200: 0.30\n",
            "loss at batch 300: 0.38\n",
            "loss at batch 400: 0.49\n",
            "Epoch 15\n",
            "loss at batch 0: 0.36\n",
            "loss at batch 100: 0.33\n",
            "loss at batch 200: 0.30\n",
            "loss at batch 300: 0.38\n",
            "loss at batch 400: 0.48\n",
            "Epoch 16\n",
            "loss at batch 0: 0.35\n",
            "loss at batch 100: 0.33\n",
            "loss at batch 200: 0.29\n",
            "loss at batch 300: 0.37\n",
            "loss at batch 400: 0.48\n",
            "Epoch 17\n",
            "loss at batch 0: 0.35\n",
            "loss at batch 100: 0.32\n",
            "loss at batch 200: 0.29\n",
            "loss at batch 300: 0.37\n",
            "loss at batch 400: 0.48\n",
            "Epoch 18\n",
            "loss at batch 0: 0.35\n",
            "loss at batch 100: 0.32\n",
            "loss at batch 200: 0.29\n",
            "loss at batch 300: 0.37\n",
            "loss at batch 400: 0.47\n",
            "Epoch 19\n",
            "loss at batch 0: 0.34\n",
            "loss at batch 100: 0.31\n",
            "loss at batch 200: 0.29\n",
            "loss at batch 300: 0.36\n",
            "loss at batch 400: 0.47\n",
            "Epoch 20\n",
            "loss at batch 0: 0.34\n",
            "loss at batch 100: 0.31\n",
            "loss at batch 200: 0.28\n",
            "loss at batch 300: 0.36\n",
            "loss at batch 400: 0.47\n",
            "Epoch 21\n",
            "loss at batch 0: 0.34\n",
            "loss at batch 100: 0.31\n",
            "loss at batch 200: 0.28\n",
            "loss at batch 300: 0.36\n",
            "loss at batch 400: 0.47\n",
            "Epoch 22\n",
            "loss at batch 0: 0.33\n",
            "loss at batch 100: 0.31\n",
            "loss at batch 200: 0.28\n",
            "loss at batch 300: 0.36\n",
            "loss at batch 400: 0.47\n",
            "Epoch 23\n",
            "loss at batch 0: 0.33\n",
            "loss at batch 100: 0.30\n",
            "loss at batch 200: 0.28\n",
            "loss at batch 300: 0.35\n",
            "loss at batch 400: 0.46\n",
            "Epoch 24\n",
            "loss at batch 0: 0.33\n",
            "loss at batch 100: 0.30\n",
            "loss at batch 200: 0.28\n",
            "loss at batch 300: 0.35\n",
            "loss at batch 400: 0.46\n",
            "Epoch 25\n",
            "loss at batch 0: 0.33\n",
            "loss at batch 100: 0.30\n",
            "loss at batch 200: 0.27\n",
            "loss at batch 300: 0.35\n",
            "loss at batch 400: 0.46\n",
            "Epoch 26\n",
            "loss at batch 0: 0.32\n",
            "loss at batch 100: 0.29\n",
            "loss at batch 200: 0.27\n",
            "loss at batch 300: 0.35\n",
            "loss at batch 400: 0.46\n",
            "Epoch 27\n",
            "loss at batch 0: 0.32\n",
            "loss at batch 100: 0.29\n",
            "loss at batch 200: 0.27\n",
            "loss at batch 300: 0.35\n",
            "loss at batch 400: 0.46\n",
            "Epoch 28\n",
            "loss at batch 0: 0.32\n",
            "loss at batch 100: 0.29\n",
            "loss at batch 200: 0.27\n",
            "loss at batch 300: 0.35\n",
            "loss at batch 400: 0.45\n",
            "Epoch 29\n",
            "loss at batch 0: 0.32\n",
            "loss at batch 100: 0.29\n",
            "loss at batch 200: 0.27\n",
            "loss at batch 300: 0.34\n",
            "loss at batch 400: 0.45\n",
            "Epoch 30\n",
            "loss at batch 0: 0.32\n",
            "loss at batch 100: 0.29\n",
            "loss at batch 200: 0.27\n",
            "loss at batch 300: 0.34\n",
            "loss at batch 400: 0.45\n",
            "Epoch 31\n",
            "loss at batch 0: 0.31\n",
            "loss at batch 100: 0.28\n",
            "loss at batch 200: 0.26\n",
            "loss at batch 300: 0.34\n",
            "loss at batch 400: 0.45\n",
            "Epoch 32\n",
            "loss at batch 0: 0.31\n",
            "loss at batch 100: 0.28\n",
            "loss at batch 200: 0.26\n",
            "loss at batch 300: 0.34\n",
            "loss at batch 400: 0.45\n",
            "Epoch 33\n",
            "loss at batch 0: 0.31\n",
            "loss at batch 100: 0.28\n",
            "loss at batch 200: 0.26\n",
            "loss at batch 300: 0.34\n",
            "loss at batch 400: 0.44\n",
            "Epoch 34\n",
            "loss at batch 0: 0.31\n",
            "loss at batch 100: 0.28\n",
            "loss at batch 200: 0.26\n",
            "loss at batch 300: 0.34\n",
            "loss at batch 400: 0.44\n",
            "Epoch 35\n",
            "loss at batch 0: 0.31\n",
            "loss at batch 100: 0.28\n",
            "loss at batch 200: 0.26\n",
            "loss at batch 300: 0.33\n",
            "loss at batch 400: 0.44\n",
            "Epoch 36\n",
            "loss at batch 0: 0.30\n",
            "loss at batch 100: 0.27\n",
            "loss at batch 200: 0.26\n",
            "loss at batch 300: 0.33\n",
            "loss at batch 400: 0.44\n",
            "Epoch 37\n",
            "loss at batch 0: 0.30\n",
            "loss at batch 100: 0.27\n",
            "loss at batch 200: 0.26\n",
            "loss at batch 300: 0.33\n",
            "loss at batch 400: 0.44\n",
            "Epoch 38\n",
            "loss at batch 0: 0.30\n",
            "loss at batch 100: 0.27\n",
            "loss at batch 200: 0.26\n",
            "loss at batch 300: 0.33\n",
            "loss at batch 400: 0.44\n",
            "Epoch 39\n",
            "loss at batch 0: 0.30\n",
            "loss at batch 100: 0.27\n",
            "loss at batch 200: 0.25\n",
            "loss at batch 300: 0.33\n",
            "loss at batch 400: 0.44\n",
            "Epoch 40\n",
            "loss at batch 0: 0.30\n",
            "loss at batch 100: 0.27\n",
            "loss at batch 200: 0.25\n",
            "loss at batch 300: 0.33\n",
            "loss at batch 400: 0.43\n",
            "Epoch 41\n",
            "loss at batch 0: 0.30\n",
            "loss at batch 100: 0.27\n",
            "loss at batch 200: 0.25\n",
            "loss at batch 300: 0.33\n",
            "loss at batch 400: 0.43\n",
            "Epoch 42\n",
            "loss at batch 0: 0.29\n",
            "loss at batch 100: 0.26\n",
            "loss at batch 200: 0.25\n",
            "loss at batch 300: 0.33\n",
            "loss at batch 400: 0.43\n",
            "Epoch 43\n",
            "loss at batch 0: 0.29\n",
            "loss at batch 100: 0.26\n",
            "loss at batch 200: 0.25\n",
            "loss at batch 300: 0.33\n",
            "loss at batch 400: 0.43\n",
            "Epoch 44\n",
            "loss at batch 0: 0.29\n",
            "loss at batch 100: 0.26\n",
            "loss at batch 200: 0.25\n",
            "loss at batch 300: 0.32\n",
            "loss at batch 400: 0.43\n",
            "Epoch 45\n",
            "loss at batch 0: 0.29\n",
            "loss at batch 100: 0.26\n",
            "loss at batch 200: 0.25\n",
            "loss at batch 300: 0.32\n",
            "loss at batch 400: 0.43\n",
            "Epoch 46\n",
            "loss at batch 0: 0.29\n",
            "loss at batch 100: 0.26\n",
            "loss at batch 200: 0.25\n",
            "loss at batch 300: 0.32\n",
            "loss at batch 400: 0.43\n",
            "Epoch 47\n",
            "loss at batch 0: 0.29\n",
            "loss at batch 100: 0.26\n",
            "loss at batch 200: 0.25\n",
            "loss at batch 300: 0.32\n",
            "loss at batch 400: 0.43\n",
            "Epoch 48\n",
            "loss at batch 0: 0.29\n",
            "loss at batch 100: 0.26\n",
            "loss at batch 200: 0.25\n",
            "loss at batch 300: 0.32\n",
            "loss at batch 400: 0.42\n",
            "Epoch 49\n",
            "loss at batch 0: 0.28\n",
            "loss at batch 100: 0.26\n",
            "loss at batch 200: 0.25\n",
            "loss at batch 300: 0.32\n",
            "loss at batch 400: 0.42\n",
            "Epoch 50\n",
            "loss at batch 0: 0.28\n",
            "loss at batch 100: 0.25\n",
            "loss at batch 200: 0.24\n",
            "loss at batch 300: 0.32\n",
            "loss at batch 400: 0.42\n",
            "Epoch 51\n",
            "loss at batch 0: 0.28\n",
            "loss at batch 100: 0.25\n",
            "loss at batch 200: 0.24\n",
            "loss at batch 300: 0.32\n",
            "loss at batch 400: 0.42\n",
            "Epoch 52\n",
            "loss at batch 0: 0.28\n",
            "loss at batch 100: 0.25\n",
            "loss at batch 200: 0.24\n",
            "loss at batch 300: 0.32\n",
            "loss at batch 400: 0.42\n",
            "Epoch 53\n",
            "loss at batch 0: 0.28\n",
            "loss at batch 100: 0.25\n",
            "loss at batch 200: 0.24\n",
            "loss at batch 300: 0.32\n",
            "loss at batch 400: 0.42\n",
            "Epoch 54\n",
            "loss at batch 0: 0.28\n",
            "loss at batch 100: 0.25\n",
            "loss at batch 200: 0.24\n",
            "loss at batch 300: 0.32\n",
            "loss at batch 400: 0.42\n",
            "Epoch 55\n",
            "loss at batch 0: 0.28\n",
            "loss at batch 100: 0.25\n",
            "loss at batch 200: 0.24\n",
            "loss at batch 300: 0.32\n",
            "loss at batch 400: 0.42\n",
            "Epoch 56\n",
            "loss at batch 0: 0.28\n",
            "loss at batch 100: 0.25\n",
            "loss at batch 200: 0.24\n",
            "loss at batch 300: 0.32\n",
            "loss at batch 400: 0.42\n",
            "Epoch 57\n",
            "loss at batch 0: 0.28\n",
            "loss at batch 100: 0.25\n",
            "loss at batch 200: 0.24\n",
            "loss at batch 300: 0.31\n",
            "loss at batch 400: 0.42\n",
            "Epoch 58\n",
            "loss at batch 0: 0.27\n",
            "loss at batch 100: 0.25\n",
            "loss at batch 200: 0.24\n",
            "loss at batch 300: 0.31\n",
            "loss at batch 400: 0.41\n",
            "Epoch 59\n",
            "loss at batch 0: 0.27\n",
            "loss at batch 100: 0.25\n",
            "loss at batch 200: 0.24\n",
            "loss at batch 300: 0.31\n",
            "loss at batch 400: 0.41\n",
            "Epoch 60\n",
            "loss at batch 0: 0.27\n",
            "loss at batch 100: 0.24\n",
            "loss at batch 200: 0.24\n",
            "loss at batch 300: 0.31\n",
            "loss at batch 400: 0.41\n",
            "Epoch 61\n",
            "loss at batch 0: 0.27\n",
            "loss at batch 100: 0.24\n",
            "loss at batch 200: 0.24\n",
            "loss at batch 300: 0.31\n",
            "loss at batch 400: 0.41\n",
            "Epoch 62\n",
            "loss at batch 0: 0.27\n",
            "loss at batch 100: 0.24\n",
            "loss at batch 200: 0.24\n",
            "loss at batch 300: 0.31\n",
            "loss at batch 400: 0.41\n",
            "Epoch 63\n",
            "loss at batch 0: 0.27\n",
            "loss at batch 100: 0.24\n",
            "loss at batch 200: 0.24\n",
            "loss at batch 300: 0.31\n",
            "loss at batch 400: 0.41\n",
            "Epoch 64\n",
            "loss at batch 0: 0.27\n",
            "loss at batch 100: 0.24\n",
            "loss at batch 200: 0.24\n",
            "loss at batch 300: 0.31\n",
            "loss at batch 400: 0.41\n",
            "Epoch 65\n",
            "loss at batch 0: 0.27\n",
            "loss at batch 100: 0.24\n",
            "loss at batch 200: 0.23\n",
            "loss at batch 300: 0.31\n",
            "loss at batch 400: 0.41\n",
            "Epoch 66\n",
            "loss at batch 0: 0.27\n",
            "loss at batch 100: 0.24\n",
            "loss at batch 200: 0.23\n",
            "loss at batch 300: 0.31\n",
            "loss at batch 400: 0.41\n",
            "Epoch 67\n",
            "loss at batch 0: 0.26\n",
            "loss at batch 100: 0.24\n",
            "loss at batch 200: 0.23\n",
            "loss at batch 300: 0.31\n",
            "loss at batch 400: 0.41\n",
            "Epoch 68\n",
            "loss at batch 0: 0.26\n",
            "loss at batch 100: 0.24\n",
            "loss at batch 200: 0.23\n",
            "loss at batch 300: 0.31\n",
            "loss at batch 400: 0.41\n",
            "Epoch 69\n",
            "loss at batch 0: 0.26\n",
            "loss at batch 100: 0.24\n",
            "loss at batch 200: 0.23\n",
            "loss at batch 300: 0.31\n",
            "loss at batch 400: 0.41\n",
            "Epoch 70\n",
            "loss at batch 0: 0.26\n",
            "loss at batch 100: 0.24\n",
            "loss at batch 200: 0.23\n",
            "loss at batch 300: 0.31\n",
            "loss at batch 400: 0.40\n",
            "Epoch 71\n",
            "loss at batch 0: 0.26\n",
            "loss at batch 100: 0.24\n",
            "loss at batch 200: 0.23\n",
            "loss at batch 300: 0.31\n",
            "loss at batch 400: 0.40\n",
            "Epoch 72\n",
            "loss at batch 0: 0.26\n",
            "loss at batch 100: 0.23\n",
            "loss at batch 200: 0.23\n",
            "loss at batch 300: 0.31\n",
            "loss at batch 400: 0.40\n",
            "Epoch 73\n",
            "loss at batch 0: 0.26\n",
            "loss at batch 100: 0.23\n",
            "loss at batch 200: 0.23\n",
            "loss at batch 300: 0.31\n",
            "loss at batch 400: 0.40\n",
            "Epoch 74\n",
            "loss at batch 0: 0.26\n",
            "loss at batch 100: 0.23\n",
            "loss at batch 200: 0.23\n",
            "loss at batch 300: 0.30\n",
            "loss at batch 400: 0.40\n",
            "Epoch 75\n",
            "loss at batch 0: 0.26\n",
            "loss at batch 100: 0.23\n",
            "loss at batch 200: 0.23\n",
            "loss at batch 300: 0.30\n",
            "loss at batch 400: 0.40\n",
            "Epoch 76\n",
            "loss at batch 0: 0.26\n",
            "loss at batch 100: 0.23\n",
            "loss at batch 200: 0.23\n",
            "loss at batch 300: 0.30\n",
            "loss at batch 400: 0.40\n",
            "Epoch 77\n",
            "loss at batch 0: 0.26\n",
            "loss at batch 100: 0.23\n",
            "loss at batch 200: 0.23\n",
            "loss at batch 300: 0.30\n",
            "loss at batch 400: 0.40\n",
            "Epoch 78\n",
            "loss at batch 0: 0.26\n",
            "loss at batch 100: 0.23\n",
            "loss at batch 200: 0.23\n",
            "loss at batch 300: 0.30\n",
            "loss at batch 400: 0.40\n",
            "Epoch 79\n",
            "loss at batch 0: 0.25\n",
            "loss at batch 100: 0.23\n",
            "loss at batch 200: 0.23\n",
            "loss at batch 300: 0.30\n",
            "loss at batch 400: 0.40\n",
            "Epoch 80\n",
            "loss at batch 0: 0.25\n",
            "loss at batch 100: 0.23\n",
            "loss at batch 200: 0.23\n",
            "loss at batch 300: 0.30\n",
            "loss at batch 400: 0.40\n",
            "Epoch 81\n",
            "loss at batch 0: 0.25\n",
            "loss at batch 100: 0.23\n",
            "loss at batch 200: 0.23\n",
            "loss at batch 300: 0.30\n",
            "loss at batch 400: 0.40\n",
            "Epoch 82\n",
            "loss at batch 0: 0.25\n",
            "loss at batch 100: 0.23\n",
            "loss at batch 200: 0.23\n",
            "loss at batch 300: 0.30\n",
            "loss at batch 400: 0.40\n",
            "Epoch 83\n",
            "loss at batch 0: 0.25\n",
            "loss at batch 100: 0.23\n",
            "loss at batch 200: 0.23\n",
            "loss at batch 300: 0.30\n",
            "loss at batch 400: 0.40\n",
            "Epoch 84\n",
            "loss at batch 0: 0.25\n",
            "loss at batch 100: 0.23\n",
            "loss at batch 200: 0.23\n",
            "loss at batch 300: 0.30\n",
            "loss at batch 400: 0.39\n",
            "Epoch 85\n",
            "loss at batch 0: 0.25\n",
            "loss at batch 100: 0.23\n",
            "loss at batch 200: 0.23\n",
            "loss at batch 300: 0.30\n",
            "loss at batch 400: 0.39\n",
            "Epoch 86\n",
            "loss at batch 0: 0.25\n",
            "loss at batch 100: 0.23\n",
            "loss at batch 200: 0.23\n",
            "loss at batch 300: 0.30\n",
            "loss at batch 400: 0.39\n",
            "Epoch 87\n",
            "loss at batch 0: 0.25\n",
            "loss at batch 100: 0.23\n",
            "loss at batch 200: 0.23\n",
            "loss at batch 300: 0.30\n",
            "loss at batch 400: 0.39\n",
            "Epoch 88\n",
            "loss at batch 0: 0.25\n",
            "loss at batch 100: 0.23\n",
            "loss at batch 200: 0.23\n",
            "loss at batch 300: 0.30\n",
            "loss at batch 400: 0.39\n",
            "Epoch 89\n",
            "loss at batch 0: 0.25\n",
            "loss at batch 100: 0.23\n",
            "loss at batch 200: 0.23\n",
            "loss at batch 300: 0.30\n",
            "loss at batch 400: 0.39\n",
            "Epoch 90\n",
            "loss at batch 0: 0.25\n",
            "loss at batch 100: 0.22\n",
            "loss at batch 200: 0.23\n",
            "loss at batch 300: 0.30\n",
            "loss at batch 400: 0.39\n",
            "Epoch 91\n",
            "loss at batch 0: 0.25\n",
            "loss at batch 100: 0.22\n",
            "loss at batch 200: 0.23\n",
            "loss at batch 300: 0.30\n",
            "loss at batch 400: 0.39\n",
            "Epoch 92\n",
            "loss at batch 0: 0.24\n",
            "loss at batch 100: 0.22\n",
            "loss at batch 200: 0.22\n",
            "loss at batch 300: 0.30\n",
            "loss at batch 400: 0.39\n",
            "Epoch 93\n",
            "loss at batch 0: 0.24\n",
            "loss at batch 100: 0.22\n",
            "loss at batch 200: 0.22\n",
            "loss at batch 300: 0.30\n",
            "loss at batch 400: 0.39\n",
            "Epoch 94\n",
            "loss at batch 0: 0.24\n",
            "loss at batch 100: 0.22\n",
            "loss at batch 200: 0.22\n",
            "loss at batch 300: 0.30\n",
            "loss at batch 400: 0.39\n",
            "Epoch 95\n",
            "loss at batch 0: 0.24\n",
            "loss at batch 100: 0.22\n",
            "loss at batch 200: 0.22\n",
            "loss at batch 300: 0.30\n",
            "loss at batch 400: 0.39\n",
            "Epoch 96\n",
            "loss at batch 0: 0.24\n",
            "loss at batch 100: 0.22\n",
            "loss at batch 200: 0.22\n",
            "loss at batch 300: 0.30\n",
            "loss at batch 400: 0.39\n",
            "Epoch 97\n",
            "loss at batch 0: 0.24\n",
            "loss at batch 100: 0.22\n",
            "loss at batch 200: 0.22\n",
            "loss at batch 300: 0.30\n",
            "loss at batch 400: 0.39\n",
            "Epoch 98\n",
            "loss at batch 0: 0.24\n",
            "loss at batch 100: 0.22\n",
            "loss at batch 200: 0.22\n",
            "loss at batch 300: 0.30\n",
            "loss at batch 400: 0.39\n",
            "Epoch 99\n",
            "loss at batch 0: 0.24\n",
            "loss at batch 100: 0.22\n",
            "loss at batch 200: 0.22\n",
            "loss at batch 300: 0.29\n",
            "loss at batch 400: 0.39\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype(\"float32\") / 255\n",
        "\n",
        "fit(model, train_images, train_labels, epochs=100, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7eqoxBeEB_y"
      },
      "source": [
        "### 評估模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JV42kbSAEB_y",
        "outputId": "6eb2ea11-f186-4e77-d399-4915068fdfe6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 0.91\n"
          ]
        }
      ],
      "source": [
        "import numpy as np \n",
        "predictions = model(test_images)\n",
        "predictions = predictions.numpy()\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "matches = predicted_labels == test_labels\n",
        "print(f\"accuracy: {matches.mean():.2f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "vqOZJ27jEB_d",
        "Ac7a0mToEB_e",
        "Vi5pQGP9EB_h",
        "UzCEjsaXBty0",
        "bQ65rehdBxAN",
        "6zbXqL_-EB_k",
        "MI4h03l6EB_m",
        "rnPmUD6sEB_n",
        "GhId3BLaEB_t",
        "stvm8hhJEB_w",
        "qLJWkfYkEB_x",
        "K7eqoxBeEB_y"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3.10.8 ('tf2')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "ef8ab87b9b534aeac9d3b69b806a42a5bb2ddd0e87978ed2609b10fbe8a47363"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}